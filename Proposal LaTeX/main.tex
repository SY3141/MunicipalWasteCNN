\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 14 Progress Report:\\Municipal Waste Image Classifier}


\author{Sunny Yao, Aswin Kuganesan, Third Author \\
  \texttt{\{yaoh24,macid2,macid3\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

\hspace*{1em} The rapid increase in municipal solid waste has become a 
major global issue, contributing to landfill overflow, 
environmental pollution, and the depletion of natural 
resources. Effective waste management plays a vital role 
in addressing these challenges by promoting recycling, 
conserving resources, and reducing the overall 
environmental footprint. However, traditional methods 
of waste sorting are often manual, time-consuming, and 
error-prone, limiting the efficiency and scalability of 
recycling systems.

This report presents the development of a Municipal Waste 
Image Classification system that applies deep learning 
techniques to automate the identification and 
categorization of waste materials. The system is designed 
to classify common waste types such as plastic, paper, 
glass, metal, and organic matter based on image data. 
By automating the classification process, the project 
aims to improve recycling efficiency, streamline waste 
management operations, and support environmental 
sustainability efforts.

The main challenge of this project lies in the 
variability and complexity of real-world waste images. 
Factors such as lighting conditions, object overlap, 
background noise, and material degradation can make 
accurate classification difficult. Overcoming these 
challenges requires the use of robust machine learning 
models capable of handling diverse input data while 
maintaining high accuracy and efficiency. The success 
of this project would demonstrate the potential of 
artificial intelligence to transform waste management 
practices and contribute to the creation of more 
sustainable and environmentally responsible cities.

\section{Related Work}

Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.

\section{Dataset}

The dataset used is the Garbage Images Dataset from Kaggle. 
The dataset contains different types of garbage in folders, 
which allows the model to get trained on the different 
types of waste. The dataset is separated into cardboard, 
glass, metal, paper, plastic and trash. Several preprocessing 
operations were employed, which involved traversing each 
subfolder in the GarbageDataset class and collecting the paths 
for all images along with their corresponding class labels. 

A list of tuples was created to store the path and label 
for all images that had a valid image format of jpg, jpeg 
or png, and the images were all loaded in RGB format. 
Using PyTorch's torchvision library, the transforms module 
resized the images to 224 by 224, which is ideal for 
image processing for the neural network while also 
maintaining image quality. 

The transforms module also converted the images to a 
tensor, which normalizes the images to a [0.0, 1.0] 
range. One percent of the dataset is then used using 
an 80 percent training and 20 percent testing split, 
since one percent of the dataset is sufficient. The 
dataset was already annotated since the folders included 
the garbage type, so matching the images to their class 
label did not require any manual annotation. Each garbage 
type was assigned to an integer value, which was mapped 
to each image path.

\section{Features}
The dataset used in this project consists of labeled 
images of municipal waste categorized into classes such 
as plastic, paper, glass, metal, and organic waste. Each 
image is organized within a directory structure, where 
each subfolder represents one waste category. A custom 
GarbageDataset class was developed to efficiently load 
the data by reading image paths and their corresponding 
labels, returning (image, label) pairs suitable for use 
in PyTorch data loaders.

Before being fed into the model, all images undergo a 
series of preprocessing steps using 
torchvision.transforms. These include resizing the 
images to a uniform size, converting them into tensors, 
and normalizing pixel values to ensure consistent scale 
and distribution across the dataset. This preprocessing 
helps improve training stability and ensures that the 
model can handle variations in lighting, background, 
and image quality.

The model itself is a Convolutional Neural Network, 
which is particularly well-suited for image 
classification tasks. Unlike traditional approaches 
that rely on manual feature engineering, the CNN 
automatically learns to extract hierarchical visual 
features directly from raw image data. Early 
convolutional layers capture low-level features such as 
edges, colors, and textures, while deeper layers identify 
higher-level patterns like shapes and object structures. 
The network then uses these learned embeddings to 
classify each image into its corresponding waste 
category.

All feature extraction and classification are performed within this single end-to-end neural network pipeline. By leveraging the CNNâ€™s ability to learn rich spatial representations, the system avoids manual feature selection and instead learns robust, generalizable features directly from data. This approach enhances classification accuracy and makes the model more adaptable to real-world waste sorting applications, where visual variability is common.
\section{Implementation}

Describe your model and implementation here. Refer to item 4. This may take around a page.

\section{Results and Evaluation}

The dataset was divided using an 80/20 split, with 80 percent 
being used for training and 20 percent being used for 
testing. One percent of the dataset was used, and the model 
was trained for 15 epochs using cross-entropy loss to determine 
the difference between the predicted class and the true label. 

Using this methodology, the training loss for the first epoch 
was 1.57, and the accuracy was 57.03 percent. The testing loss 
was 0.22 for the first epoch, and the accuracy was 100 percent. 
However, in the second epoch and afterwards, the training loss 
and testing loss are 0, and the training and testing accuracy 
are 100 percent. 

Although this demonstrates that the model is able to classify 
the waste properly, this also demonstrates that the model is 
overfitting to the dataset. The baseline is the neural network 
trained on the garbage dataset, which should be adjusted to 
account for overfitting, and the baseline performance is 100 
percent, which is high due to overfitting.  

\section{Feedback and Plans}

For the remainder of the project, our primary focus will 
be on improving the accuracy, robustness, and 
generalization ability of our CNN-based waste 
classification model. We will also explore other CNN
based models to look at their techniques and see if we can
incorporate them into our own model.

\section{Template Notes}

You can remove this section or comment it out, as it only contains instructions for how to use this template. You may use subsections in your document as you find appropriate.

\subsection{Tables and figures}

See Table~\ref{citation-guide} for an example of a table and its caption.
See Figure~\ref{fig:experiments} for an example of a figure and its caption.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{example-image-golden}
  \caption{A figure with a caption that runs for more than one line.
    Example image is usually available through the \texttt{mwe} package
    without even mentioning it in the preamble.}
  \label{fig:experiments}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
  \includegraphics[width=0.48\linewidth]{example-image-b}
  \caption {A minimal working example to demonstrate how to place
    two images side-by-side.}
\end{figure*}


\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.
This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
