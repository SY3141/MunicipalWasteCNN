\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Group 14 Progress Report:\\Municipal Waste Image Classifier}


\author{Sunny Yao, Aswin Kuganesan, Third Author \\
  \texttt{\{yaoh24,macid2,macid3\}@mcmaster.ca} }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

\section{Introduction}

\hspace*{1em} The rapid increase in municipal solid waste has become a 
major global issue, contributing to landfill overflow, 
environmental pollution, and the depletion of natural 
resources. Effective waste management plays a vital role 
in addressing these challenges by promoting recycling, 
conserving resources, and reducing the overall 
environmental footprint. However, traditional methods 
of waste sorting are often manual, time-consuming, and 
error-prone, limiting the efficiency and scalability of 
recycling systems.

This report presents the development of a Municipal Waste 
Image Classification system that applies deep learning 
techniques to automate the identification and 
categorization of waste materials. The system is designed 
to classify common waste types such as plastic, paper, 
glass, metal, and organic matter based on image data. 
By automating the classification process, the project 
aims to improve recycling efficiency, streamline waste 
management operations, and support environmental 
sustainability efforts.

The main challenge of this project lies in the 
variability and complexity of real-world waste images. 
Factors such as lighting conditions, object overlap, 
background noise, and material degradation can make 
accurate classification difficult. Overcoming these 
challenges requires the use of robust machine learning 
models capable of handling diverse input data while 
maintaining high accuracy and efficiency. The success 
of this project would demonstrate the potential of 
artificial intelligence to transform waste management 
practices and contribute to the creation of more 
sustainable and environmentally responsible cities.

\section{Related Work}

Image classification for waste management has gained significant attention in recent research, with Convolutional Neural Networks (CNNs) proving particularly effective due to their ability to automatically learn hierarchical feature representations from image data.
Wang et al. (2019) developed a waste classification system using transfer learning with pre-trained models like VGG16 and ResNet50, achieving over 90\% accuracy on multi-class waste datasets. Their work demonstrated the effectiveness of deep features for distinguishing between waste categories including plastic, paper, and metal.
Yang and Thung (2016) created one of the first comprehensive waste classification datasets and implemented a custom CNN architecture for six waste categories similar to our work. They reported challenges with distinguishing visually similar materials and emphasized the importance of data augmentation and proper preprocessing.
Awe et al. (2020) proposed an ensemble approach combining multiple CNN architectures, highlighting the difficulty in classifying categories that share visual similarities, such as different types of plastics or metal objects. Kumar et al. (2021) investigated lightweight CNN architectures for edge device deployment, balancing model complexity with inference speed for practical real-time sorting applications.
Our approach builds upon these works by implementing a custom CNN architecture with progressive feature extraction through multiple convolutional blocks, incorporating global average pooling to reduce overfitting, and utilizing dropout for regularization. While our dataset is currently limited in size, the model architecture follows proven design principles from the literature while being optimized for the six-category waste classification task.

\section{Dataset}

The dataset used is the Garbage Images Dataset from Kaggle. 
The dataset contains different types of garbage in folders, 
which allows the model to get trained on the different 
types of waste. The dataset is separated into cardboard, 
glass, metal, paper, plastic and trash. Several preprocessing 
operations were employed, which involved traversing each 
subfolder in the GarbageDataset class and collecting the paths 
for all images along with their corresponding class labels. 

A list of tuples was created to store the path and label 
for all images that had a valid image format of jpg, jpeg 
or png, and the images were all loaded in RGB format. 
Using PyTorch's torchvision library, the transforms module 
resized the images to 224 by 224, which is ideal for 
image processing for the neural network while also 
maintaining image quality. 

The transforms module also converted the images to a 
tensor, which normalizes the images to a [0.0, 1.0] 
range. One percent of the dataset is then used using 
an 80 percent training and 20 percent testing split, 
since one percent of the dataset is sufficient. The 
dataset was already annotated since the folders included 
the garbage type, so matching the images to their class 
label did not require any manual annotation. Each garbage 
type was assigned to an integer value, which was mapped 
to each image path.

\section{Features}
The dataset used in this project consists of labeled 
images of municipal waste categorized into classes such 
as plastic, paper, glass, metal, and organic waste. Each 
image is organized within a directory structure, where 
each subfolder represents one waste category. A custom 
GarbageDataset class was developed to efficiently load 
the data by reading image paths and their corresponding 
labels, returning (image, label) pairs suitable for use 
in PyTorch data loaders.

Before being fed into the model, all images undergo a 
series of preprocessing steps using 
torchvision.transforms. These include resizing the 
images to a uniform size, converting them into tensors, 
and normalizing pixel values to ensure consistent scale 
and distribution across the dataset. This preprocessing 
helps improve training stability and ensures that the 
model can handle variations in lighting, background, 
and image quality.

The model itself is a Convolutional Neural Network, 
which is particularly well-suited for image 
classification tasks. Unlike traditional approaches 
that rely on manual feature engineering, the CNN 
automatically learns to extract hierarchical visual 
features directly from raw image data. Early 
convolutional layers capture low-level features such as 
edges, colors, and textures, while deeper layers identify 
higher-level patterns like shapes and object structures. 
The network then uses these learned embeddings to 
classify each image into its corresponding waste 
category.

All feature extraction and classification are performed within this single end-to-end neural network pipeline. By leveraging the CNN’s ability to learn rich spatial representations, the system avoids manual feature selection and instead learns robust, generalizable features directly from data. This approach enhances classification accuracy and makes the model more adaptable to real-world waste sorting applications, where visual variability is common.

\section{Implementation}

Our waste classification model is implemented as a PyTorch neural network comprising four convolutional blocks followed by a classification head. The architecture is designed to progressively extract increasingly complex features from input images while maintaining computational efficiency.

\textbf{Network Architecture:}

The model accepts RGB images of size 224×224 pixels (3 input channels) and processes them through four sequential convolutional blocks:

Block 1 consists of two 3×3 convolutional layers with 64 filters each, followed by ReLU activations and 2×2 max pooling. This block captures low-level features such as edges and simple textures. The use of padding = 1 preserves spatial dimensions before pooling.

Block 2 doubles the feature depth to 128 channels through two convolutional layers, again followed by ReLU activations and max pooling. This block learns mid-level representations that combine basic shapes and textures typical of different waste materials.

Block 3 increases feature depth to 256 channels with a single 3×3 convolution and max pooling, enabling the network to detect higher-order visual patterns related to object composition and structure.

Block 4 expands the feature space to 512 channels through a 3×3 convolutional layer followed by ReLU activation and 2×2 max pooling. This final block consolidates high-level abstractions and provides rich, discriminative features for classification. Including this block improves the model’s capacity to handle the visual diversity in waste imagery without excessive overfitting.

Following the convolutional stages, we employ Global Average Pooling (AdaptiveAvgPool2d) rather than standard flattening. This reduces the number of parameters substantially by averaging each feature map into a single scalar, improving generalization on limited data.


The classification head consists of:
\begin{itemize}
    \item A flattening layer to convert 2D features to 1D
    \item A fully connected layer reducing 256 features to 256 hidden units with ReLU activation
    \item A dropout layer (p=0.5) for regularization during training
    \item A final linear layer mapping to 6 output classes (cardboard, glass, metal, paper, plastic, trash)
\end{itemize}

\textbf{Training Configuration:}

The model is trained using the Adam optimizer with a learning rate of 0.001 and Cross-Entropy loss function, which is appropriate for multi-class classification. We employ stratified train-test splitting (80/20) to ensure balanced class representation, with a subset of 1\% of the full dataset (139 samples) used for computational efficiency during development.

Data preprocessing includes resizing all images to 224×224 pixels and conversion to tensors. The batch size is set to 32 for both training and testing. Our training loop implements 15 epochs with validation after each epoch to monitor performance.

\textbf{Performance Characteristics:}

The implementation uses PyTorch 2.9.0 and is designed for easy extension to GPU acceleration, though current development is CPU-based. The modular architecture allows for straightforward modifications such as adjusting the number of filters, adding batch normalization, or implementing different pooling strategies.


\section{Results and Evaluation}

The dataset was divided using an 80/20 split, with 80 percent 
being used for training and 20 percent being used for 
testing. One percent of the dataset was used, and the model 
was trained for 15 epochs using cross-entropy loss to determine 
the difference between the predicted class and the true label. 

Using this methodology, the training loss for the first epoch 
was 1.57, and the accuracy was 57.03 percent. The testing loss 
was 0.22 for the first epoch, and the accuracy was 100 percent. 
However, in the second epoch and afterwards, the training loss 
and testing loss are 0, and the training and testing accuracy 
are 100 percent. 

Although this demonstrates that the model is able to classify 
the waste properly, this also demonstrates that the model is 
overfitting to the dataset. The baseline is the neural network 
trained on the garbage dataset, which should be adjusted to 
account for overfitting, and the baseline performance is 100 
percent, which is high due to overfitting.  

\section{Feedback and Plans}

For the remainder of the project, our primary focus will 
be on improving the accuracy, robustness, and 
generalization ability of our CNN-based waste 
classification model. We will also explore other CNN
based models to look at their techniques and see if we can
incorporate them into our own model.

\section{Template Notes}

\subsection{Tables and figures}

See Table~\ref{citation-guide} for an example of a table and its caption.
See Figure~\ref{fig:experiments} for an example of a figure and its caption.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{example-image-golden}
  \caption{A figure with a caption that runs for more than one line.
    Example image is usually available through the \texttt{mwe} package
    without even mentioning it in the preamble.}
  \label{fig:experiments}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
  \includegraphics[width=0.48\linewidth]{example-image-b}
  \caption {A minimal working example to demonstrate how to place
    two images side-by-side.}
\end{figure*}


\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

Many websites where you can find academic papers also allow you to export a bib file for citation or bib formatted entry. Copy this into the \texttt{custom.bib} and you will be able to cite the paper in the \LaTeX{}. You can remove the example entries.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.
This an example cross-reference to Equation~\ref{eq:example}. You can also write equations inline, like this: $A=\pi r^2$.


% \section*{Limitations}

\section*{Team Contributions}

Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}

% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
